import ast
import itertools
import logging
import os
from pathlib import Path

import pandas as pd

from risk_policy_distillation.datasets.abs_dataset import AbstractDataset
from risk_policy_distillation.explanation.bipartite_graph import BipartiteGraph
from risk_policy_distillation.models.explainers.global_explainers.global_expl import (
    GlobalExplainer,
)
from risk_policy_distillation.pipeline.clusterer import Clusterer
from risk_policy_distillation.pipeline.concept_extractor import Extractor


logger = logging.getLogger("logger")


class Pipeline:

    def __init__(
        self,
        extractor: Extractor = None,
        clusterer: Clusterer = None,
        concept_dataset: pd.DataFrame = None,
        lime=True,
        fr=True,
        verbose=0,
    ):
        """
        Pipeline for generating local and global explanations.
        :param extractor: an Extractor object representing local explanation generation
        :param clusterer: a Clusterer object representing global explanation generation
        :param concept_dataset: a AbstractDataset object
        :param lime: If the pipeline uses a local word-based explainer like LIME to verify local concept-based explanations
        :param fr: If the pipeline uses a factuality assessor like FactReasoner to verify global explanations
        :param verbose: amount of logs generated -- 0 for very little logging, 1 for practically everything logged.
        """
        self.extractor = extractor
        self.clusterer = clusterer

        self.concept_dataset = concept_dataset
        self.lime = lime
        self.fr = fr

        self.verbose = verbose

        logger.info("Built pipeline.")
        logger.info("Using LIME = {}".format(self.lime))
        logger.info("Using FactReasoner = {}".format(self.fr))

    # TODO: think about passing the guardian model to the run() instead of through Extractor
    def run(
        self, dataset: AbstractDataset, results_path: Path = Path("results")
    ) -> GlobalExplainer:
        """
        Runs the full pipeline for local and global explanation generation.
        :param dataset: an AbstractDataset or one of its subclasses
        :param path: a global path to the results folder within which local and global explanations will be saved.
        :return: global explanation for the dataset generated by the pipeline.
        """
        # generate paths for saving the explanations
        local_expl_path, global_expl_path = self.generate_output_folders(
            results_path, dataset.dataset_name
        )

        logger.info(
            f"Results directory for {dataset.dataset_name}: [{local_expl_path}, {global_expl_path}]"
        )

        # if the global explanation already exists load it
        if global_expl_path.exists():
            global_expl = GlobalExplainer(expl_path=global_expl_path)
            return global_expl

        if self.concept_dataset is None:
            if self.extractor is not None:
                # generate local explanations by extracting concepts
                self.extractor.extract_concepts(
                    dataset, local_expl_path, self.lime, self.verbose
                )
                self.concept_dataset = pd.read_csv(local_expl_path, header=0)
            else:
                raise ValueError(
                    "No extractor or concept dataset provided for the pipeline."
                )

        # load concepts into a bipartite graph format
        expl_graph = self.get_graph_expl(
            concept_dataset=self.concept_dataset, labels=self.extractor.guardian.labels
        )

        # run clustering to reduce the size of the graph
        if self.clusterer is not None:
            best_graph = self.clusterer.run_clustering(
                graph=expl_graph,
                labels=self.extractor.guardian.labels,
                use_fr=self.fr,
                verbose=self.verbose,
            )

            # get a global explanation from the summarized graph
            global_expl = GlobalExplainer(expl_graph=best_graph)
            global_expl.save(global_expl_path)

            return global_expl

    def custom_zipp(self, concepts):
        max_len = max(len(c) for c in concepts)

        if not max_len:
            return []

        for c in concepts:
            if len(c) < max_len:
                c += ["none"] * (max_len - len(c))

        combinations = list(set(itertools.product(*concepts)))

        return combinations

    def get_graph_expl(self, concept_dataset, labels) -> BipartiteGraph:
        """
        Convert a dataset of local concept-based explanations into a bipartite graph
        :param concept_dataset: a dataset of concept-based explanations generated by the Extractor
        :param labels: a list of labels of the task. Each label will correspond to a single partition of the graph.
        :return: A BipartiteGraph object representing a graph loaded with concepts.
        """
        expl_graph = BipartiteGraph(labels)
        label_names = self.extractor.guardian.label_names

        concept_datasets = []
        for i in labels:
            cs = concept_dataset[concept_dataset["GG Label"] == i]
            concept_datasets.append(cs)

        for i, cds in enumerate(concept_datasets):
            if not len(cds):
                continue

            cds["Zipped"] = cds.apply(
                lambda x: self.custom_zipp(
                    [ast.literal_eval(x[label_names[d]]) for d in labels]
                ),
                axis=1,
            )

            concept_pairs = cds["Zipped"].values.tolist()
            flattened_concepts = [item for sublist in concept_pairs for item in sublist]
            expl_graph.load_graph(flattened_concepts, label=i)

        return expl_graph

    def generate_output_folders(self, results_path: Path, dataset_name):
        """
        Generates two paths for storing local and global explanations generated by the pipeline.
        :param path: Absolute path to a results folder
        :param dataset_name: The name of the dataset being explained
        :return: paths to local and global explanations
        """

        local_expl_path = results_path.joinpath(dataset_name, "local")
        global_expl_path = results_path.joinpath(dataset_name, "global")

        create_folders = [local_expl_path, global_expl_path]
        for dir_path in create_folders:
            if not dir_path.is_dir():
                os.makedirs(dir_path)

        local_expl_path = local_expl_path.joinpath("local_expl.csv")
        global_expl_path = global_expl_path.joinpath("global_expl.pkl")

        return local_expl_path, global_expl_path
