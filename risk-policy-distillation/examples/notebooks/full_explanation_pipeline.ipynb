{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92b30a6-4a3c-4215-a0b8-3f61480ccba3",
   "metadata": {},
   "source": [
    "# Generating global explanations of LLM-as-a-Judge using GloVE algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e8712c-f06a-4bad-970b-fdcf85c3aa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhaval/.pyenv/versions/policy-dist/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-30 12:07:41 [__init__.py:216] Automatically detected platform cpu.\n"
     ]
    }
   ],
   "source": [
    "from risk_policy_distillation.datasets.prompt_response_dataset import (\n",
    "    PromptResponseDataset,\n",
    ")\n",
    "from risk_policy_distillation.datasets.abs_dataset import AbstractDataset\n",
    "from risk_policy_distillation.models.explainers.local_explainers.lime import LIME\n",
    "from risk_policy_distillation.models.explainers.local_explainers.shap_vals import SHAP\n",
    "from risk_policy_distillation.models.guardians.guardian import Guardian\n",
    "from risk_policy_distillation.pipeline.clusterer import Clusterer\n",
    "from risk_policy_distillation.pipeline.concept_extractor import Extractor\n",
    "from risk_policy_distillation.pipeline.pipeline import Pipeline\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2b11f-d205-4f81-93bb-9ab3dd785db8",
   "metadata": {},
   "source": [
    "#### Create a dataset\n",
    "\n",
    "To explain the LLM-as-a-Judge you need to provide a dataset. [AbstractDataset](../src/models/datasets/abs_dataset.py) class provides a wrapper for a dataframe you want to explain. You can use [PromptDataset](../src/models/datasets/prompt_dataset.py) or [PromptResponseDataset](../src/models/datasets/prompt_response_dataset.py) depending on whether your dataframe consists of only prompts or prompt-response pairs. You can also create a custom dataset by inheriting the Dataset class.\n",
    "\n",
    "You have to provide a config with information on column name mapping. Additional parameters: _flip_labels_ indicates whether labels of the dataframe should be flipped in preprocessing step (e.g. for BeaverTails where labels indicate that the content is safe rather than harmful); _split_ indicates whether a train-val-test split needs to be performed during preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bc4d137-23e8-47da-afbd-88947bc07643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "ds = load_dataset(\"PKU-Alignment/BeaverTails\")\n",
    "dataframe = ds.data[\"330k_train\"].table.to_pandas()\n",
    "\n",
    "# sample for testing purposes\n",
    "dataframe = dataframe.sample(3)\n",
    "\n",
    "# provide some information in the data config\n",
    "data_config = {\n",
    "    \"dataset_name\": \"BeaverTails\",\n",
    "    \"index_col\": \"\",  # if there is no index column it will be automatically populated\n",
    "    \"prompt_col\": \"prompt\",\n",
    "    \"response_col\": \"response\",\n",
    "    \"label_col\": \"is_safe\",\n",
    "}\n",
    "\n",
    "dataset = PromptResponseDataset(dataframe=dataframe, config=data_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c952984",
   "metadata": {},
   "source": [
    "### Create a guardian config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ef0055-4e03-4ed5-9806-b3fc3609cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_config = {\n",
    "    \"task\": f\"Harm detection\",\n",
    "    \"criterion\": f\"Harm\",\n",
    "    \"criterion_definition\": f\"The message has harmful by common-sense.\",\n",
    "    \"labels\": [0, 1],\n",
    "    \"label_names\": [\"harmless\", \"harmful\"],\n",
    "    \"output_labels\": [\"no\", \"yes\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0aa6a",
   "metadata": {},
   "source": [
    "##### Policy Distillation uses granite gurdian as a LLM-as-a-Judge, and other Large Language Models (LLMs) to query at various stages in the pipeline. Therefore requires access to inference or call the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from typing import Dict, List, Union, Any, Optional\n",
    "import dataclasses\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from tqdm.autonotebook import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "def run_parallel(func, items, concurrency_limit: int = 10, verbose=True):\n",
    "    outputs = []\n",
    "    with ThreadPool(processes=concurrency_limit) as pool:\n",
    "        for output in tqdm(\n",
    "            pool.imap(func, items), total=len(items), disable=(not verbose)\n",
    "        ):\n",
    "            outputs.append(output)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(kw_only=True)\n",
    "class TextGenerationInferenceOutput:\n",
    "\n",
    "    prediction: Union[str, List[Dict[str, Any]]]\n",
    "    logprobs: Optional[Dict[str, float]] = None\n",
    "\n",
    "\n",
    "class RITSInferenceEngine:\n",
    "\n",
    "    def __init__(self, model_name_or_path, parameters={}):\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.parameters = parameters\n",
    "        model_name_for_endpoint = (\n",
    "            model_name_or_path.split(\"/\")[-1].lower().replace(\".\", \"-\")\n",
    "        )\n",
    "        self.model = OpenAI(\n",
    "            api_key=\"RITS_API_KEY\",\n",
    "            base_url=f\"https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com/{model_name_for_endpoint}/v1\",\n",
    "            default_headers={\"RITS_API_KEY\": \"RITS_API_KEY\"},\n",
    "        )\n",
    "\n",
    "    def chat(\n",
    "        self,\n",
    "        messages,\n",
    "        response_format=None,\n",
    "        postprocessors=None,\n",
    "    ) -> TextGenerationInferenceOutput:\n",
    "\n",
    "        def chat_response(messages):\n",
    "            response = self.model.chat.completions.create(\n",
    "                messages=messages,\n",
    "                model=self.model_name_or_path,\n",
    "                response_format=self._create_schema_format(response_format),\n",
    "                **self.parameters,\n",
    "            )\n",
    "            return self._prepare_chat_output(response, postprocessors)\n",
    "\n",
    "        return run_parallel(chat_response, messages)\n",
    "\n",
    "    def _prepare_chat_output(self, response, postprocessors):\n",
    "        return TextGenerationInferenceOutput(\n",
    "            prediction=(\n",
    "                json.loads(response.choices[0].message.content)\n",
    "                if postprocessors\n",
    "                else response.choices[0].message.content\n",
    "            ),\n",
    "            logprobs=(\n",
    "                {\n",
    "                    output.token.strip(): output.logprob\n",
    "                    for output in response.choices[0].logprobs.content\n",
    "                }\n",
    "                if response.choices[0].logprobs\n",
    "                else None\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def _create_schema_format(self, response_format):\n",
    "        if response_format:\n",
    "            return {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"RITS_schema\",\n",
    "                    \"schema\": response_format,\n",
    "                },\n",
    "            }\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "guardian_judge = RITSInferenceEngine(\n",
    "    model_name_or_path=\"ibm-granite/granite-guardian-3.3-8b\",\n",
    "    parameters={\"logprobs\": True, \"top_logprobs\": 10, \"temperature\": 0.0},\n",
    ")\n",
    "\n",
    "llm_component = RITSInferenceEngine(\n",
    "    model_name_or_path=\"meta-llama/llama-3-3-70b-instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d419d3d-922e-4e10-aefe-3e8617e65e01",
   "metadata": {},
   "source": [
    "### Create and run the explanation generation pipeline\n",
    "\n",
    "Pipeline streamlines local and global explanation generation process. Extractor executes the CLoVE algorithm and generates a set of local explanations, and Clusterer executes GloVE algorithm and merges the local explanations into a global one.\n",
    "\n",
    "Pass `lime=False` to pipeline creation step if no local word-based verification is done. SImilarly, use `fr=False` if FactReasoner is not used to verify global explanations.\n",
    "\n",
    "The resulting local and global explanations are saved in the path folder passed to the pipeline.run() call.\n",
    "The execution logs can be found in the logs folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aee994e0-9e5d-4903-81af-22c78c240fb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n",
      "100%|██████████| 100/100 [00:04<00:00, 22.38it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n",
      "100%|██████████| 100/100 [00:04<00:00, 21.73it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n",
      "100%|██████████| 100/100 [00:05<00:00, 19.29it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
      "3it [00:37, 12.39s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 74235.47prompts/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 39199.10prompts/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 24966.10prompts/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 73584.28prompts/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 64527.75prompts/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 27685.17prompts/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 30727.50prompts/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 25575.02prompts/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 58661.59prompts/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 51150.05prompts/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 22192.08prompts/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 25040.62prompts/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 32896.50prompts/s]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 53773.13prompts/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 28926.23prompts/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 28532.68prompts/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 31300.78prompts/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 31300.78prompts/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 31184.42prompts/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RITS = True Model = llama-3.1-8b-instruct\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI: 100%|██████████| 2/2 [00:00<00:00, 31300.78prompts/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rules': [{'prediction': 1, 'if_clause': 'involves race issues', 'despite_clauses': ['provides factual information']}, {'prediction': 1, 'if_clause': 'suggests eating animal waste', 'despite_clauses': ['provides factual information']}, {'prediction': 1, 'if_clause': 'promotes vigilantism', 'despite_clauses': ['provides factual information']}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the guardian model\n",
    "guardian = Guardian(\n",
    "    inference_engine=guardian_judge,\n",
    "    config=guardian_config,\n",
    ")\n",
    "\n",
    "local_expl = \"LIME\"\n",
    "# local explanation model -- only LIME and SHAP are supported\n",
    "if local_expl == \"LIME\":\n",
    "    local_explainer = LIME(\n",
    "        dataset.dataset_name, guardian_config[\"label_names\"], n_samples=100\n",
    "    )\n",
    "elif local_expl == \"SHAP\":\n",
    "    local_explainer = SHAP(\n",
    "        dataset.dataset_name, guardian_config[\"label_names\"], n_samples=100\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Only LIME and SHAP are supported\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(\n",
    "    extractor=Extractor(\n",
    "        guardian,\n",
    "        llm_component,\n",
    "        guardian_config[\"criterion\"],\n",
    "        guardian_config[\"criterion_definition\"],\n",
    "        local_explainer,\n",
    "    ),\n",
    "    clusterer=Clusterer(\n",
    "        llm_component,\n",
    "        guardian_config[\"criterion_definition\"],\n",
    "        guardian_config[\"label_names\"],\n",
    "        n_iter=10,\n",
    "    ),\n",
    "    lime=True,\n",
    "    fr=True,\n",
    ")\n",
    "\n",
    "# Run pipeline\n",
    "expl = pipeline.run(dataset)\n",
    "\n",
    "print(expl.print())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
