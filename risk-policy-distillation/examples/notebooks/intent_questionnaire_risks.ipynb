{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "- Generate data as a set of intents\n",
    "- Run through the pipeline to get a set of (initial) risk prioritizations\n",
    "- Create LLM-as-a-Judge to reprioritize each risk\n",
    "- For each risk run the policy distillation -> obtain a policy for when this risk should be reprioritized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seshu/Documents/2025/policy-distillation-ran-extension/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from risk_policy_distillation.llms.rits_component import RITSComponent\n",
    "# from risk_policy_distillation.llms.ollama_component import OllamaComponent\n",
    "from dotenv import load_dotenv\n",
    "from langextract.resolver import ResolverParsingError\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import ast\n",
    "import math\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import langextract as lx\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # setting up logging\n",
    "# logger = logging.getLogger('logger')\n",
    "# logger.setLevel(logging.INFO)\n",
    "# fh = logging.FileHandler(f'logs/{datetime.datetime.now().strftime(\"%m_%d__%H_%M\")}.log')\n",
    "# fh.setLevel(logging.INFO)\n",
    "# logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seshu/Documents/2025/policy-distillation-ran-extension/.venv/lib/python3.11/site-packages/langextract/inference.py:32: FutureWarning: `langextract.inference.OllamaLanguageModel` is deprecated and will be removed in v2.0.0; use `langextract.providers.ollama.OllamaLanguageModel` instead.\n",
      "  return inference.__getattr__(name)\n"
     ]
    }
   ],
   "source": [
    "class model_type_timeout(lx.inference.OllamaLanguageModel):\n",
    "    def _ollama_query(self,*args,**kwargs):\n",
    "        kwargs.setdefault('timeout',120)\n",
    "        return super()._ollama_query(*args,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a synthetic dataset of intents\n",
    "# Use an LLM to generate a set of intents as scenarios where AI can be used\n",
    "def generate_intent_data(llm_component, n_samples=10):\n",
    "    context = \"\"\"\n",
    "                You are a helpful AI assistant that can generate diverse AI usecase scenarios.\n",
    "              \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "                Generate a list of {n_samples} scenarios in which AI could be used to automate or replaces specific tasks. \n",
    "                Include information about how this tool will be used or monitored.\n",
    "\n",
    "                Here are some examples of scenarios:\n",
    "                    - AI medical chatbot for internal use. Ouptuts monitored by medical professionals. Not used for diagnostics or patient care.\n",
    "                    - AI-enhanced interior design tool that helps users design their homes. Used by people decorating their homes. Verified for hallucinatory behavior.\n",
    "                    - Storytelling app for kids where you can choose characters and different plot features and AI generates a story. Guardrails for toxic and harmful ouptut are implemented.\n",
    "\n",
    "                Return ONLY the list of scenarios formatted as a Python list\n",
    "                Do not include any additional information or explanations.\n",
    "            \"\"\" \n",
    "\n",
    "    intents = llm_component.send_request(context, prompt)\n",
    "    intents = ast.literal_eval(intents)\n",
    "\n",
    "    print(len(intents))\n",
    "    \n",
    "    return intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts structured information about the intent such as AI tasks, domain, stakeholders\n",
    "def extract_intent_features(input_text):\n",
    "    # 1. Define the prompt and extraction rules\n",
    "    prompt = \"\"\"\n",
    "        Get the answers only to the following questions. Make sure to provide the mentioned attributes for all questions.\n",
    "        Classify the confidence attribute as \"Direct answer from the input text\" or \"Likely answer from the  input text\" or \"Unable to answer from input text\". \n",
    "    \n",
    "        1. \"What domain does your use request fall under amongst the following options?\n",
    "        Customer service/support, Technical, Information retrieval, Strategy, Code/software engineering, Communications, IT/business automation, Writing assistant, Financial,\n",
    "        Talent and Organization including HR, Product, Marketing, Cybersecurity, Healthcare, User Research, Sales, Risk and Compliance, Design, Other\", attribute: domain. \n",
    "        There must be a direct or likely answer for this question. \n",
    "        2. In which environment is the system used?, attribute: environment\n",
    "        3. Is there private data for this use-case?, attribute: private data\n",
    "        4. What techniques are utilised in the system? Multi-modal: {Document Question/Answering, Image and text to text, Video and text to text, visual question answering}, \n",
    "                                                       Natural language processing: {feature extraction, fill mask, question answering, sentence similarity, summarization, table question answering, text classification, text generation, token classification, translation, zero shot classification},\n",
    "                                                       computer vision: {image classification, image segmentation, text to image, object detection}, audio:{audio classification, audio to audio, text to speech}, tabular: {tabular classification, tabular regression}, \n",
    "                                                       reinforcement learning                     \n",
    "            attribute: ai_techniques. There must be a direct or likely answer for this question. \n",
    "        5. Who is the subject as per the intent?\n",
    "        \"\"\"\n",
    "    \n",
    "    # 2. Provide a high-quality example to guide the model\n",
    "    examples = [\n",
    "        lx.data.ExampleData(\n",
    "            text=\"Generate personalized, relevant responses, recommendations, and summaries of claims for customers to support agents to enhance their interactions with customers. LLM has been robustly tested and shows high accuracy with no bias or fairness issues. There are guardrais for privacy and security. The main subjects are customers/claimants (whose data and preferences the LLM uses), support agents (who receive the personalized content), and the organization’s stakeholders (compliance, security, and tech teams) who oversee the system’s integrity\",\n",
    "            extractions=[\n",
    "                lx.data.Extraction(\n",
    "                    extraction_class=\"What techniques are utilised in the system from the given options? Multi-modal: {Document Question/Answering, Image and text to text, Video and text to text, visual question answering}, Natural language processing: {feature extraction, fill mask, question answering, sentence similarity, summarization, table question answering, text classification, text generation, token classification, translation, zero shot classification}, computer vision: {image classification, image segmentation, text to image, object detection}, audio:{audio classification, audio to audio, text to speech}, tabular: {tabular classification, tabular regression}, reinforcement learning\",\n",
    "                    extraction_text=\"1. Generate personalized, relevant responses 2. summaries of claims \",\n",
    "                    attributes={\"confidence\": \"Likely answer from the  input text\", \"ai_techniques\": \"Natural language processing: {Text classification, sentence similarity, question answering, summarization, text generation} \"}\n",
    "                ),\n",
    "                lx.data.Extraction(\n",
    "                    extraction_class=\"What domain does your use request fall under? Customer service/support, Technical, Information retrieval, Strategy, Code/software engineering, Communications, IT/business automation, Writing assistant, Financial, Talent and Organization including HR, Product, Marketing, Cybersecurity, Healthcare, User Research, Sales, Risk and Compliance, Design, Other\",\n",
    "                    extraction_text=\"summaries of claims for customers to support agents to enhance their interactions with customers\",\n",
    "                    attributes={\"confidence\": \"Directly from the  input text\", \"domain\": \"Customer service/support\"}\n",
    "                ),\n",
    "                lx.data.Extraction(\n",
    "                    extraction_class=\"In which environment is the system used?\",\n",
    "                    extraction_text=\" \",\n",
    "                    attributes={\"confidence\": \"Likely answer from the  input text\", \"environment\": \"digital environment, specifically designed for online interactions between customers and support agents\"}\n",
    "                ),\n",
    "                lx.data.Extraction(\n",
    "                    extraction_class=\"Is there private data for this use-case?\",\n",
    "                    extraction_text=\" \",\n",
    "                    attributes={\"confidence\": \"Unable to answer from  input text\", \"private_data\": \"Unable to answer from the text\" }\n",
    "                ),\n",
    "                lx.data.Extraction(\n",
    "                    extraction_class=\"Who is the subject as per the intent?\",\n",
    "                    extraction_text=\"The main subjects are customers/claimants (whose data and preferences the LLM uses), support agents (who receive the personalized content), and the organization’s stakeholders (compliance, security, and tech teams) who oversee the system’s integrity\",\n",
    "                    attributes={\"confidence\": \"Likely answer from the  input text\", \"subject_data\": \"customers/claimants, support agents, compliance, security, and tech teams\" }\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    \n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end. Quantify the statement as most likely if it is not in the context. If no answer is possible from the context, state that as it is not possible to give that from the context. Succinct answer and appended to the context \\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    model = OllamaLLM(model=\"llama3:8b\")\n",
    "    chain = prompt | model\n",
    "\n",
    "    techniques = chain.invoke({\"question\":\"What techniques are utilised in the system from the given options? Multi-modal: {Document Question/Answering, Image and text to text, Video and text to text, visual question answering}, Natural language processing: {feature extraction, fill mask, question answering, sentence similarity, summarization, table question answering, text classification, text generation, token classification, translation, zero shot classification}, computer vision: {image classification, image segmentation, text to image, object detection}, audio:{audio classification, audio to audio, text to speech}, tabular: {tabular classification, tabular regression}, reinforcement learning\",\"context\": input_text})\n",
    "    input_text = input_text + techniques\n",
    "    n_tries = 0\n",
    "    while n_tries < 10:\n",
    "        try:\n",
    "            result = lx.extract(\n",
    "                        text_or_documents=input_text,\n",
    "                        prompt_description=prompt,\n",
    "                        examples=examples,\n",
    "                        language_model_type=model_type_timeout, #lx.inference.OllamaLanguageModel,\n",
    "                        model_id=\"llama3:8b\",  # or any Ollama model\n",
    "                        model_url=\"http://localhost:11435\",\n",
    "                        fence_output=False,\n",
    "                        use_schema_constraints=False)\n",
    "\n",
    "            return result\n",
    "        except ResolverParsingError:\n",
    "            n_tries += 1\n",
    "                    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ai_task_names():\n",
    "    with open(\"risk_matrix/ai_tasks_risk_matrix.json\") as f:\n",
    "        tasks = json.load(f)\n",
    "    \n",
    "    ai_tasks = []\n",
    "    \n",
    "    for task in tasks[\"tasks\"]:\n",
    "        ai_tasks.append(task[\"name\"])\n",
    "    \n",
    "    return ai_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_extraction_to_ai_risks(intent_ai_tasks, ai_tasks):\n",
    "    # Extracts AI tasks from the intent text\n",
    "    template = \"\"\"You are a ai task analysis assistant. Given the following freeform text, extract which ai tasks are PRESENT from this list: {ai_tasks} - If an ai task is explicitly mentioned as NOT present (e.g., 'no regression'), exclude it. - If close enough words are used (e.g., 'doc qnA' → 'Document Question Answering'), map them to the correct ai task. - Only return the ai tasks that are clearly PRESENT. Answer only with the ai tasks in list format. DO NOT INCLUDE any preamble or explanations \\n\\n{intent_ai_tasks}\\n:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    model = OllamaLLM(model=\"llama3:8b\")\n",
    "    \n",
    "    chain = prompt | model\n",
    "    \n",
    "    intent_ai_techniques = chain.invoke({\"ai_tasks\": ai_tasks, \"intent_ai_tasks\": intent_ai_tasks})\n",
    "\n",
    "    intent_ai_techniques = ast.literal_eval(intent_ai_techniques)\n",
    "\n",
    "    ai_task_indices = [ai_tasks.index(x.strip())  for x in intent_ai_techniques if x.strip() in ai_tasks]\n",
    "    ai_task_indices\n",
    "\n",
    "    return ai_task_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_risks(dict_list):\n",
    "    priority = {\"Low\": 1, \"Medium\": 2, \"High\": 3, \"Critical\": 4}\n",
    "    \n",
    "    result = {key: \"Low\" for key in dict_list[0].keys()}\n",
    "    \n",
    "    for d in dict_list:\n",
    "        for key, value in d.items():\n",
    "            if priority[value] > priority[result[key]]:\n",
    "                result[key] = value\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_risks(ai_task_indices):\n",
    "    # Loads baseline risks from a json \n",
    "    with open(\"risk_matrix/ai_tasks_risk_matrix.json\") as f:\n",
    "        tasks = json.load(f)\n",
    "    \n",
    "    all_baseline_risks = [tasks[\"tasks\"][index][\"baseline_risks\"] for index in ai_task_indices]\n",
    "    return all_baseline_risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n_samples=100):\n",
    "    'Generates an intent-risk-baseline priority dataset we can use to reprioritize risks'\n",
    "    try:\n",
    "        df = pd.read_csv('datasets/intents.csv', header=0)\n",
    "    except FileNotFoundError:\n",
    "        data = []\n",
    "        \n",
    "        llm_component = RITSComponent('llama-3-3-70b-instruct', 'meta-llama/llama-3-3-70b-instruct')    \n",
    "        intent_data = generate_intent_data(llm_component, n_samples=n_samples)\n",
    "        ai_tasks_names = get_ai_task_names()\n",
    "    \n",
    "        for intent_text in intent_data:\n",
    "            # extract features of the intent like stakeholders, AI tasks, domains etc\n",
    "            extraction = extract_intent_features(intent_text).extractions[0]\n",
    "\n",
    "            if extraction.attributes is None or \"ai_techniques\" not in extraction.attributes.keys():\n",
    "                continue\n",
    "    \n",
    "            # extract AI tasks present in the scenario\n",
    "            ai_task_indices = map_extraction_to_ai_risks(extraction.attributes[\"ai_techniques\"], ai_tasks_names)\n",
    "            baseline_risks = get_baseline_risks(ai_task_indices)\n",
    "    \n",
    "            final_risk_matrix = consolidate_risks(baseline_risks)\n",
    "    \n",
    "            for risk, priority in final_risk_matrix.items():\n",
    "                data.append((intent_text, risk, priority))\n",
    "\n",
    "        df = pd.DataFrame(data, columns=['intent', 'risk', 'priority'])\n",
    "        df.to_csv('datasets/intents.csv', index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_dataset(n_samples=100):\n",
    "#     'Generates an intent-risk-baseline priority dataset we can use to reprioritize risks'\n",
    "#     try:\n",
    "#         df = pd.read_csv('datasets/intents.csv', header=0)\n",
    "#     except FileNotFoundError:\n",
    "#         data = []\n",
    "        \n",
    "#         llm_component = OllamaComponent('llama-3-3-70b-instruct', 'meta-llama/llama-3-3-70b-instruct')    \n",
    "#         intent_data = generate_intent_data(llm_component, n_samples=n_samples)\n",
    "#         ai_tasks_names = get_ai_task_names()\n",
    "    \n",
    "#         for intent_text in intent_data:\n",
    "#             # extract features of the intent like stakeholders, AI tasks, domains etc\n",
    "#             extraction = extract_intent_features(intent_text).extractions[0]\n",
    "\n",
    "#             if extraction.attributes is None or \"ai_techniques\" not in extraction.attributes.keys():\n",
    "#                 continue\n",
    "    \n",
    "#             # extract AI tasks present in the scenario\n",
    "#             ai_task_indices = map_extraction_to_ai_risks(extraction.attributes[\"ai_techniques\"], ai_tasks_names)\n",
    "#             baseline_risks = get_baseline_risks(ai_task_indices)\n",
    "    \n",
    "#             final_risk_matrix = consolidate_risks(baseline_risks)\n",
    "    \n",
    "#             for risk, priority in final_risk_matrix.items():\n",
    "#                 data.append((intent_text, risk, priority))\n",
    "\n",
    "#         df = pd.DataFrame(data, columns=['intent', 'risk', 'priority'])\n",
    "#         df.to_csv('datasets/intents.csv', index=False)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from risk_policy_distillation.models.guardians.judge import Judge\n",
    "from risk_policy_distillation.utils.rits_util import post_rits_req\n",
    "\n",
    "class RiskReprioritizationJudge(Judge):\n",
    "    ''' Simple LLM-as-a-Judge model that decides whether a risk should be reprioritized'''\n",
    "    def __init__(self, model_name, model_served_name, config):\n",
    "        super().__init__(config)\n",
    "        self.model_name = model_name\n",
    "        self.model_served_name = model_served_name\n",
    "        self.context = \"\"\"\n",
    "                        Given a task, AI risk, and a baseline estimated impact of this risk on the task decide if the impact should be changed.\n",
    "                        Answer with one of the possible risk impacts: Low, Medium, High or Critical. \n",
    "\n",
    "                        Here are some examples:\n",
    "                        \n",
    "                        AI Task: Generate personalized, relevant responses, recommendations, and summaries of claims for customers to support agents to enhance their interactions with customers. \n",
    "                                 LLM has been thoroughly tested and shows high accuracy with no bias or fairness issues. \n",
    "                                 There are guardrais for privacy and security. \n",
    "                                 The main subjects are customers/claimants (whose data and preferences the LLM uses), support agents (who receive the personalized content), \n",
    "                                 and the organization’s stakeholders (compliance, security, and tech teams) who oversee the system’s integrity. \n",
    "                        Risk: Fairness\n",
    "                        Risk impact: Medium\n",
    "                        New impact: Low\n",
    "                        Supporting text: high accuracy with no bias or fairness issues\n",
    "\n",
    "                        Answer ONLY with one word indicating new estimated impact for this risk (i.e. \"Low\"/\"Medium\"/\"High\"/\"Critical\").\n",
    "                        Do not include any other explanation or information.\n",
    "                        \"\"\"\n",
    "\n",
    "        self.rits_url = f'https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com/{self.model_name}/v1/chat/completions'\n",
    "        \n",
    "    def ask_guardian(self, message):\n",
    "        prompt, risk, priority = message\n",
    "        message = f''' Message:\n",
    "                        AI task: {prompt}\n",
    "                        Risk: {risk}\n",
    "                        Risk impact: {priority}'''\n",
    "\n",
    "    \n",
    "        i = 0\n",
    "        while i < 100:\n",
    "            response = requests.post(\n",
    "                self.rits_url,\n",
    "                headers={\n",
    "                    \"RITS_API_KEY\": os.getenv('RITS_API_KEY'),\n",
    "                    \"Content-Type\": \"application/json\"},\n",
    "                json={\n",
    "                    \"model\": self.model_served_name,\n",
    "                    \"messages\": [{\"role\": \"system\", \"content\": self.context, \"name\":\"test\"}, {\"role\": \"user\", \"content\": message, \"name\":\"test\"}],\n",
    "                    \"temperature\": 0.7, \n",
    "                    \"logprobs\": True, \n",
    "                    \"top_logprobs\": 10\n",
    "                }\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                break\n",
    "            else:\n",
    "                i += 1\n",
    "                \n",
    "        answer = response.json()['choices'][0]['message']['content']\n",
    "        \n",
    "        return answer\n",
    "\n",
    "    def predict_proba(self, inputs):\n",
    "        probabilities = []\n",
    "\n",
    "        for input_text in inputs:\n",
    "            i = 0\n",
    "            while i < 100:\n",
    "                response = requests.post(\n",
    "                    self.rits_url,\n",
    "                    headers={\n",
    "                        \"RITS_API_KEY\": os.getenv('RITS_API_KEY'),\n",
    "                        \"Content-Type\": \"application/json\"},\n",
    "                    json={\n",
    "                        \"model\": self.model_served_name,\n",
    "                        \"messages\": [{\"role\": \"system\", \"content\": self.context}, {\"role\": \"user\", \"content\": input_text}],\n",
    "                        \"temperature\": 0.7, \n",
    "                        \"logprobs\": True, \n",
    "                        \"top_logprobs\": 10\n",
    "                    }\n",
    "                )\n",
    "                if response.status_code == 200:\n",
    "                    break\n",
    "                else:\n",
    "                    i += 1\n",
    "                                \n",
    "            answer = response.json()['choices'][0]['message']['content']\n",
    "            \n",
    "            log_probs = response.json()['choices'][0]['logprobs']['content'][0]['top_logprobs']\n",
    "\n",
    "            answer = response.json()['choices'][0]\n",
    "    \n",
    "            tokens = [i['token'] for i in log_probs]\n",
    "            token_probs = [i['logprob'] for i in log_probs]\n",
    "            \n",
    "            probs = [math.e ** token_probs[tokens.index(label)] if label in tokens else 0 for label in self.label_names ]\n",
    "            probabilities.append(probs)\n",
    "\n",
    "        return np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# print(os.getenv('RITS_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_config = {\n",
    "                \"task\": \"risk reprioritization\",\n",
    "                 \"criterion\": \"risk impact reevaluation\",\n",
    "                 \"criterion_definition\": \"Given a task, AI risk, and a baseline estimated impact of this risk on the task decide if the impact should be changed.\",\n",
    "                 \"labels\": [0, 1, 2, 3],\n",
    "                 \"label_names\": [\"Low\", \"Medium\", \"High\", \"Critical\"],\n",
    "                 \"output_labels\": [\"Low\", \"Medium\", \"High\", \"Critical\"]\n",
    "              }\n",
    "\n",
    "judge = RiskReprioritizationJudge('llama-3-3-70b-instruct', 'meta-llama/llama-3-3-70b-instruct', judge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from risk_policy_distillation.datasets.prompt_dataset import PromptDataset\n",
    "\n",
    "data_config = {\n",
    "            \"general\": {\n",
    "              \"location\": \"\",\n",
    "              \"dataset_name\": \"intent_dataset\"\n",
    "            },\n",
    "            \"data\": {\n",
    "              \"type\": \"prompt\",\n",
    "              \"index_col\": \"Index\",\n",
    "              \"prompt_col\": \"intent\",\n",
    "              \"label_col\": \"priority\",\n",
    "              \"flip_labels\": False\n",
    "            },\n",
    "            \"split\": {\n",
    "              \"split\": False,\n",
    "              \"subset\": \"test\",\n",
    "              \"sample_ratio\": 1\n",
    "            }\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from risk_policy_distillation.datasets.abs_dataset import AbstractDataset\n",
    "\n",
    "class IntentDataset(AbstractDataset):\n",
    "\n",
    "    def __init__(self, config, dataframe=None):\n",
    "        super().__init__(config, dataframe)\n",
    "\n",
    "        self.expl_input = self.prompt_col\n",
    "        self.message_labels = [self.prompt_col, 'risk', 'priority']\n",
    "\n",
    "    def extract_message(self, row):\n",
    "        prompt = row[self.prompt_col]\n",
    "        risk = row['risk']\n",
    "        baseline_priority = row['priority']\n",
    "        local_expl_input = prompt\n",
    "\n",
    "        # get true label\n",
    "        true_label = row[self.label_col]\n",
    "\n",
    "        return (prompt, risk, baseline_priority), local_expl_input, true_label\n",
    "\n",
    "    def build_message_format(self, message, decision):\n",
    "        prompt, risk, _ = message\n",
    "        return f''' Message:\n",
    "                        AI Task: {prompt}\n",
    "                        Risk: {risk}\n",
    "                        Risk impact: {decision}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load intent dataset\n",
    "df = pd.read_csv('datasets/intents.csv', header=0)\n",
    "dataset = IntentDataset(data_config, df[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-01 11:00:41 [__init__.py:216] Automatically detected platform cpu.\n"
     ]
    }
   ],
   "source": [
    "from risk_policy_distillation.models.explainers.local_explainers.lime import LIME\n",
    "from risk_policy_distillation.pipeline.pipeline import Pipeline\n",
    "from risk_policy_distillation.pipeline.clusterer import Clusterer\n",
    "from risk_policy_distillation.pipeline.concept_extractor import Extractor\n",
    "\n",
    "llm_component = RITSComponent('llama-3-3-70b-instruct', 'meta-llama/llama-3-3-70b-instruct')\n",
    "local_explainer = LIME('intent', judge_config['label_names'], n_samples=500, n_words=6)\n",
    "\n",
    "pipeline = Pipeline(extractor=Extractor(judge, llm_component, judge_config['criterion'],\n",
    "                                        judge_config['criterion_definition'], local_explainer),\n",
    "                    clusterer=Clusterer(llm_component, judge_config['criterion_definition'],\n",
    "                                        judge_config['label_names'], n_iter=20),\n",
    "                    lime=True,\n",
    "                    fr=True,\n",
    "                    verbose=1)\n",
    "\n",
    "# evaluate the global explanation\n",
    "path = f'results/intent_dataset'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from risk_policy_distillation.models.explainers.local_explainers.lime import LIME\n",
    "# from risk_policy_distillation.pipeline.pipeline import Pipeline\n",
    "# from risk_policy_distillation.pipeline.clusterer import Clusterer\n",
    "# from risk_policy_distillation.pipeline.concept_extractor import Extractor\n",
    "\n",
    "# llm_component = OllamaComponent('llama3.2:3b-instruct-fp16', 'llama3.2:latest')\n",
    "# local_explainer = LIME('intent', judge_config['label_names'], n_samples=500, n_words=6)\n",
    "\n",
    "# pipeline = Pipeline(extractor=Extractor(judge, llm_component, judge_config['criterion'],\n",
    "#                                         judge_config['criterion_definition'], local_explainer),\n",
    "#                     clusterer=Clusterer(llm_component, judge_config['criterion_definition'],\n",
    "#                                         judge_config['label_names'], n_iter=20),\n",
    "#                     lime=True,\n",
    "#                     fr=True,\n",
    "#                     verbose=1)\n",
    "\n",
    "# # evaluate the global explanation\n",
    "# path = f'/Users/seshu/Documents/2025/policy-distillation-ran-extension/examples/notebooks/results/intent_dataset'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  results/intent_dataset\n",
      "results/intent_dataset/intent_dataset/local/expl.csv results/intent_dataset/intent_dataset/global/global_expl.pkl\n",
      "results/intent_dataset/intent_dataset/local/expl.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 56ed98e0-b6af-43ba-978f-bcc1c0bead85)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 56ed98e0-b6af-43ba-978f-bcc1c0bead85)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
      "0it [07:50, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'IntentDataset' object has no attribute 'response_col'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/2025/policy-distillation-ran-extension/src/risk_policy_distillation/pipeline/concept_extractor.py:59\u001b[39m, in \u001b[36mExtractor.extract_concepts\u001b[39m\u001b[34m(self, dataset, save_path, use_lime, verbose)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(save_path)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m ds = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ds) == dataset.size():\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# if all inputs are processed already\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/2025/policy-distillation-ran-extension/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/2025/policy-distillation-ran-extension/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/2025/policy-distillation-ran-extension/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/2025/policy-distillation-ran-extension/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/2025/policy-distillation-ran-extension/.venv/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m     handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'results/intent_dataset/intent_dataset/local/expl.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m expl = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/2025/policy-distillation-ran-extension/src/risk_policy_distillation/pipeline/pipeline.py:79\u001b[39m, in \u001b[36mPipeline.run\u001b[39m\u001b[34m(self, dataset, path)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m.concept_dataset = \u001b[38;5;28mself\u001b[39m.concept_dataset\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# generate local explanations by extracting concepts\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_concepts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_expl_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mself\u001b[39m.concept_dataset = pd.read_csv(local_expl_path, header=\u001b[32m0\u001b[39m)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/2025/policy-distillation-ran-extension/src/risk_policy_distillation/pipeline/concept_extractor.py:77\u001b[39m, in \u001b[36mExtractor.extract_concepts\u001b[39m\u001b[34m(self, dataset, save_path, use_lime, verbose)\u001b[39m\n\u001b[32m     72\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._extract_concepts(\n\u001b[32m     73\u001b[39m             dataset, save_path, use_lime, verbose, start_id=last_id\n\u001b[32m     74\u001b[39m         )\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_concepts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_lime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/2025/policy-distillation-ran-extension/src/risk_policy_distillation/pipeline/concept_extractor.py:113\u001b[39m, in \u001b[36mExtractor._extract_concepts\u001b[39m\u001b[34m(self, dataset, save_path, use_lime, verbose, start_id)\u001b[39m\n\u001b[32m    110\u001b[39m bulletpoints = \u001b[38;5;28mself\u001b[39m.remove_redundancies(bulletpoints)\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# save results in a dataframe\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguardian_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrue_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbulletpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m    124\u001b[39m     logger.info(message)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/2025/policy-distillation-ran-extension/src/risk_policy_distillation/pipeline/concept_extractor.py:199\u001b[39m, in \u001b[36mExtractor.save_results\u001b[39m\u001b[34m(self, dataset, row, message, guardian_response, true_label, bulletpoints, save_path)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_results\u001b[39m(\n\u001b[32m    189\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    190\u001b[39m     dataset,\n\u001b[32m   (...)\u001b[39m\u001b[32m    196\u001b[39m     save_path,\n\u001b[32m    197\u001b[39m ):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m         message_names = [dataset.prompt_col, \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponse_col\u001b[49m]\n\u001b[32m    200\u001b[39m         records = [\n\u001b[32m    201\u001b[39m             [\n\u001b[32m    202\u001b[39m                 row[dataset.index_col],\n\u001b[32m   (...)\u001b[39m\u001b[32m    208\u001b[39m             + [bulletpoints[d] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.guardian.labels]\n\u001b[32m    209\u001b[39m         ]\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'IntentDataset' object has no attribute 'response_col'"
     ]
    }
   ],
   "source": [
    "expl = pipeline.run(dataset, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in enumerate(expl.rules):\n",
    "    print(f'{expl.predictions[i]} IF {r} DESPITE {expl.despites[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
